{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "blA5esJtdFVP"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rurwL-W2PoIN"
      },
      "source": [
        "## Creating Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "owY9mN1oPAal"
      },
      "outputs": [],
      "source": [
        "class DenseLayer:\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * torch.rand(n_inputs, n_neurons)\n",
        "    self.biases = torch.zeros((1, n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = torch.matmul(inputs, self.weights) + self.biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajjr0QcUP8tA"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6owCQofBP_iL"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = torch.max(torch.tensor(0),inputs)\n",
        "class Activation_Sigmoid:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = 1 / (1 + torch.exp(inputs*-1))\n",
        "class Activation_Softmax:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = torch.exp(inputs - torch.max(inputs, axis=1, keepdim=True).values)\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / torch.sum(exp_values, axis=1, keepdim=True)\n",
        "    self.output = probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7usBC_5fprz"
      },
      "source": [
        "## Loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-xWPtRUefp3j"
      },
      "outputs": [],
      "source": [
        "class Loss_CategoricalCrossentropy() :\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = torch.clip(y_pred, 1e-8, 1 - 1e-8)\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    # Mask values - only for one-hot encoded labels\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = torch.sum(y_pred_clipped * y_true, axis=1)\n",
        "    log_loss = -torch.log(correct_confidences)\n",
        "    data_loss = torch.mean(log_loss)\n",
        "    return data_loss\n",
        "class Accuracy():\n",
        "  def calculate(self, y_pred, y_true):\n",
        "    predictions = torch.argmax(y_pred, axis=1)\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = torch.argmax(y_true, axis=1)\n",
        "    accuracy = torch.mean((predictions == y_true).float())\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([0.1, 0.5])\n",
        "y = torch.tensor([0.05,0.2, 0.45, 0.3])"
      ],
      "metadata": {
        "id": "7FZ7Pk0gIK8v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_1 = DenseLayer(2, 4)\n",
        "activation1 = Activation_ReLU()\n",
        "output_layer = DenseLayer(4, 4)\n",
        "activation2 = Activation_Sigmoid()"
      ],
      "metadata": {
        "id": "Cb5W0woZRU9_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X):\n",
        "  hidden_layer_1.forward(X)\n",
        "  activation1.forward(hidden_layer_1.output)\n",
        "  output_layer.forward(activation1.output)\n",
        "  activation2.forward(output_layer.output)\n",
        "  return activation2.output"
      ],
      "metadata": {
        "id": "WAoeJRRJRArn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(fp):\n",
        "    lr = torch.tensor(0.01)\n",
        "\n",
        "    # Calculate gradients for the output layer\n",
        "    back1 = (fp[0][0] - y[0]) * (1 - fp[0][0]) * fp[0][0]\n",
        "    back2 = (fp[0][1] - y[1]) * (1 - fp[0][1]) * fp[0][1]\n",
        "    back3 = (fp[0][2] - y[2]) * (1 - fp[0][2]) * fp[0][2]\n",
        "    back4 = (fp[0][3] - y[3]) * (1 - fp[0][3]) * fp[0][3]\n",
        "\n",
        "    # Update weights and biases for the output layer\n",
        "    output_layer.weights[0][0] -= lr * back1 * activation1.output[0][0]\n",
        "    output_layer.weights[0][1] -= lr * back2 * activation1.output[0][1]\n",
        "    output_layer.weights[0][2] -= lr * back3 * activation1.output[0][2]\n",
        "    output_layer.weights[0][3] -= lr * back4 * activation1.output[0][3]\n",
        "    output_layer.biases[0][0] -= lr * back1\n",
        "    output_layer.biases[0][1] -= lr * back2\n",
        "    output_layer.biases[0][2] -= lr * back3\n",
        "    output_layer.biases[0][3] -= lr * back4\n",
        "\n",
        "    # Calculate gradients for the hidden layer\n",
        "    hidden_back1 = back1 * output_layer.weights[0][0] * (1 if hidden_layer_1.output[0][0] > 0 else 0)\n",
        "    hidden_back2 = back2 * output_layer.weights[0][1] * (1 if hidden_layer_1.output[0][0] > 0 else 0)\n",
        "    hidden_back3 = back3 * output_layer.weights[0][2] * (1 if hidden_layer_1.output[0][1] > 0 else 0)\n",
        "    hidden_back4 = back4 * output_layer.weights[0][3] * (1 if hidden_layer_1.output[0][1] > 0 else 0)\n",
        "\n",
        "    # Update weights and biases for the hidden layer\n",
        "    hidden_layer_1.weights[0][0] -= lr * (hidden_back1 * X[0] + hidden_back2 * X[0])\n",
        "    hidden_layer_1.weights[0][1] -= lr * (hidden_back1 * X[1] + hidden_back2 * X[1])\n",
        "    hidden_layer_1.weights[1][0] -= lr * (hidden_back3 * X[0] + hidden_back4 * X[0])\n",
        "    hidden_layer_1.weights[1][1] -= lr * (hidden_back3 * X[1] + hidden_back4 * X[1])\n",
        "    hidden_layer_1.biases[0][0] -= lr * (hidden_back1 + hidden_back3)\n",
        "    hidden_layer_1.biases[0][1] -= lr * (hidden_back2 + hidden_back4)\n"
      ],
      "metadata": {
        "id": "cJePM510S1HH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_calculation(y_true, y_pred):\n",
        "  return torch.mean(0.5*(y_true - y_pred)**2)"
      ],
      "metadata": {
        "id": "RzJ7jkxaTs8D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNKQBlGWTgDz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = 0.0001\n",
        "y_pred = forward_pass(X)\n",
        "err = error_calculation(y, y_pred)\n",
        "print(\"Initial loss:\", err)\n",
        "print(\"Initial prediction:\",y_pred)\n",
        "rotation = 0\n",
        "while err > loss and rotation < 100000:\n",
        "  back_prop(y_pred)\n",
        "  y_pred = forward_pass(X)\n",
        "  err = error_calculation(y, y_pred)\n",
        "  rotation += 1\n",
        "print(\"Final loss:\", err)\n",
        "print(\"Final prediction:\",y_pred)\n",
        "print(\"Target value:\",y)"
      ],
      "metadata": {
        "id": "taOHtItDTR28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27b812b-30e4-47bf-d762-df7e6ceae1a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: tensor(9.9994e-05)\n",
            "Initial prediction: tensor([[0.0783, 0.2006, 0.4500, 0.3000]])\n",
            "Final loss: tensor(9.9994e-05)\n",
            "Final prediction: tensor([[0.0783, 0.2006, 0.4500, 0.3000]])\n",
            "Target value: tensor([0.0500, 0.2000, 0.4500, 0.3000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = Accuracy()\n",
        "accuracy = acc.calculate(y_pred, y.unsqueeze(0))\n",
        "print(\"The accuracy is:\", accuracy)"
      ],
      "metadata": {
        "id": "chb9Oj0cCU2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717c4937-6347-41be-9bd1-7b5455774940"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is: tensor(1.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}